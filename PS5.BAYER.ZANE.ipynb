{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS5.BAYER.ZANE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Set 5 - Zane Bayer - CSCI 520 - 5/6/2022"
      ],
      "metadata": {
        "id": "kg2F7HslrtHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Learning From Data*\n",
        "\n"
      ],
      "metadata": {
        "id": "sNp_0h-6r317"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex. 1.7\n",
        "\n",
        "### a. the hypothesis g will predict a value of 1 for the three points. \n",
        "\n",
        "Agree 3: $f_8$\n",
        "\n",
        "Agree 2: $f_4, f_6, f_7$\n",
        "\n",
        "Agree 1: $f_2, f_3, f_5$\n",
        "\n",
        "Agree 0: $f_1$\n",
        "\n",
        "### b. the hypothesis g will predict a value of 0 for the three points.\n",
        "\n",
        "Agree 3: $f_1$\n",
        "\n",
        "Agree 2: $f_2, f_3, f_5$\n",
        "\n",
        "Agree 1: $f_4, f_6, f_7$\n",
        "\n",
        "Agree 0: $f_8$\n",
        "\n",
        "### c. g(101)=0, g(110)=0, g(111)=1\n",
        "\n",
        "Agree 3: $f_2$\n",
        "\n",
        "Agree 2: $f_1, f_4, f_6$\n",
        "\n",
        "Agree 1: $f_3, f_5, f_8$\n",
        "\n",
        "Agree 0: $f_7$\n",
        "\n",
        "\n",
        "### d. g(101)=1, g(110)=1, g(111)=0\n",
        "\n",
        "Agree 3: $f_7$\n",
        "\n",
        "Agree 2: $f_3, f_5, f_8$\n",
        "\n",
        "Agree 1: $f_1, f_4, f_6$\n",
        "\n",
        "Agree 0: $f_2$\n",
        "\n"
      ],
      "metadata": {
        "id": "XMP1ncwxH9hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 1.8\n",
        "### Since $œÖ‚â§0.1$ and we know 10 marbles were drawn, then either 0 or 1 red marbles were selected. So the probability of $œÖ\\le 0.1$ is\n",
        "### <center> $P(œÖ ‚â§ 0.1)=P(r=0)+P(r=1)$\n",
        "\n",
        "### The probabilites on the RHS side can be found using the binomial distribution which states that the probability of an event X with probability p occuring k times over n trials is \n",
        "### <center> $P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}$\n",
        "### where $\\binom{n}{k}$ is the binomial coeffiecient. This gives us\n",
        "### <center> $P(œÖ ‚â§ 0.1)=(1)(.9)(.1)^{10}+10(.9)(.1)^{9}=9.09¬∑10^{-9}$"
      ],
      "metadata": {
        "id": "uM1vf7FuPoB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 1.9\n",
        "\n",
        "### Hoeffding's Inequality is\n",
        "### <center> $P[|œÖ-Œº|>œµ]‚â§2e^{-2œµ^2N}$\n",
        "###We are told that $œÖ\\le 0.1$, $\\mu=.9$, $N=10$. First, it's clear that \n",
        "### <center> $œÖ\\le 0.1 ‚áí\\mu-œÖ\\ge \\mu-.1 ‚áí Œº-œÖ‚â•.8$\n",
        "### So \n",
        "### <center> $P(œÖ‚â§.1) = P(| Œº-œÖ|‚â•.8)$\n",
        "### Then by Hoeffding's Inequality with $œµ=.75$, we get\n",
        "### <center> $P(œÖ‚â§.1) ‚â§ P(| Œº-œÖ|>.75)$\n",
        "### <center> $P(œÖ‚â§.1) ‚â§ 2e^{-2œµ^2N}$\n",
        "### <center> $P(œÖ‚â§.1) ‚â§ 2.60146 ¬∑ 10^5$\n"
      ],
      "metadata": {
        "id": "8vbZ_-V-aUE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 1.11\n",
        "\n",
        "###a. No, we cannot garuntee that $S$ will [erform better than random outside the data set. There is always the possibility that $D$ misrepresents the true data, so we can never garuntee the performance of $S$ on the true data will be greater than the probability $p$. \n",
        "\n",
        "### b. yes, if the data set $S$ misrepresents the true function, then $C$ would perform better than $S$. For instance, say that $p>.5$,but most of the points in $D$ are -1. The $C$ algorithm would perform better in this scenario.\n",
        "\n",
        "### c. As discussed in the part b, $C$ will outperform $S$ on the true data when the sample $D $'misrepresents' the data set for $p>.5$. In other words, this occurs when $p>.5$, but the sample frequency is $\\upsilon < .5$. Let's compute the probablity of this occuring using Hoeffding's Inequality.\n",
        "### <center> $$P[|œÖ-Œº|>œµ]‚â§2e^{-2œµ^2N}$$\n",
        "### The most likely scenario is $p=0.9$ and $\\upsilon=0.49$, so let's use $œµ=0.41$.\n",
        "### <center> $P[|œÖ-Œº|>0.41]‚â§2e^{-2(0.41)^2(25)}$\n",
        "### <center> $P[|œÖ-Œº|>0.41]‚â§4.47 ¬∑10^{-4}$\n",
        "### This the probability that the dataset $D$ 'misrepresents' the true function so that the hypothesis $C$ outperforms $S$ in the most likely scenario of $œÖ=0.49$ and $p=.9$. As $œÖ$ decreases further (i.e. $D$ is more inaccurate), $œµ$ will grow and the probability will rapidly decrease meaning it's less and less likely that $C$ will outperform $S$.\n",
        "\n",
        "### d. No, Hoeffding's Inequality tells us that for alll values of $p$, it's likely that $œÖ$ will be close to $p$ and $S$ will outperform $C$.\n"
      ],
      "metadata": {
        "id": "ATUnYwvKjh3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 1.12\n",
        "\n",
        "### the best which can be promised is likely option c:\n",
        "\n",
        "### One of two things will happen:\n",
        "*   You will produce a hypothesis $g$\n",
        "*   You will declare that you failed\n",
        "### If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample. \n",
        "\n",
        "### I believe this is the best promise which can be made because we know next to nothing about $f$ or the level of accuracy which our friend would like us to approximate $f$ to. It may turn out that $f$ is not learnable or completely random, it which case we would fail. However, if $f$ is sufficiently well behaved, then Hoeffding's Inequallity tells us we can approximate $f$ to a high level of accuracy given 4000 samples.\n"
      ],
      "metadata": {
        "id": "dopndhvcpiEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 1.13\n",
        "\n",
        "### We are told \n",
        "### <center> $P(y|\\vec{x})=Œª$ for $y=f(\\vec{x})$\n",
        "### <center> $P(y|\\vec{x})=1-Œª$ for $y \\neq f(\\vec{x})$\n",
        "\n",
        "### a. we're asked to find the probability of error, which is $P(h \\neq y)$. this can be done directly \n",
        "### <center> $P(h \\neq y=P(h \\neq y|y=f(x))P(y=f(x))+P(h \\neq y|y \\neq f(x))P(y \\neq f(x))$\n",
        "### were told that h makes has an error rate in approximating $f(x)$ of $Œº$, so\n",
        "### <center> $P(h \\neq y)=\\mu\\lambda+(1-\\mu)(1-Œª)$\n",
        "### <center> $=\\mu(2Œª-1)+(1-Œª)$\n",
        "### This is the desired expression.\n",
        "### b. From part a, it's clear that for $\\lambda=0.5$ we get\n",
        "### <center>  $P(h \\neq y)=(1-Œª)=0.5$\n",
        "### which is independent of $\\mu$."
      ],
      "metadata": {
        "id": "XAjOJT3opmDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EX. 3.6\n",
        "\n",
        "### a. For a hypothesis $h(x)$, the liklihood is given by \n",
        "### <center> $P(y|\\vec{x})=h(x)$ for $y=1$\n",
        "### <center> $P(y|\\vec{x})=1-h(x)$ for $y=-1$\n",
        "### The max liklihood is then given by\n",
        "### <center> $E_{in}(\\vec{w})=-\\sum_{n=1}^N ln(P(y_n,\\vec{x}_n))$\n",
        "### Since we have two distinct cases for $y=-1$ and $y=1$, we can  seperate tis sum into two parts.\n",
        "### <center> $E_{in}(\\vec{w})=-\\sum_{n=1}^N \\{[y_n=1]ln(h(x))+[y_n=-1]ln(1-h(x))\\}\n",
        "$\n",
        "### <center>  $E_{in}(\\vec{w})=\\sum_{n=1}^N \\{[y_n=1]ln(\\frac{1}{h(x)})+[y_n=-1]ln(\\frac{1}{1-h(x)})\\}\n",
        "$\n",
        "\n",
        "### which is the desired result.\n",
        "\n",
        "### b. For $h(\\vec{x})=Œ∏(\\vec{w}^T\\vec{x})$, the above equation becomes\n",
        "### <center> $E_{in}(\\vec{w})=\\sum_{n=1}^N \\{[y_n=1]ln(\\frac{1}{Œ∏(\\vec{w}^T\\vec{x})})+[y_n=-1]ln(\\frac{1}{1-Œ∏(\\vec{w}^T\\vec{x})})\\}$\n",
        "\n",
        "### The logarithm terms become $ln(\\frac{1}{Œ∏(\\vec{w}^T\\vec{x})})=ln(1+e^{-\\vec{w}^T\\vec{x}_n})$ and $ln(\\frac{1}{1-Œ∏(\\vec{w}^T\\vec{x})})=ln(1+e^{\\vec{w}^T\\vec{x}_n})$, so we get\n",
        "\n",
        "### <center> $\\sum_{n=1}^N \\{[y_n=1]ln(1+e^{-\\vec{w}^T\\vec{x}_n})+[y_n=1]ln(1+e^{\\vec{w}^T\\vec{x}_n})\\}$\n",
        "### <center> $\\sum_{n=1}^N ln(1+e^{-y_n\\vec{w}^T\\vec{x}_n})$\n",
        "### which is the dsired result of eq. 3.9.\n"
      ],
      "metadata": {
        "id": "U-u9rGb1po56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 3.10\n",
        "\n",
        "###a. The change in weights an iteration of the SGD algorithm with error measure $e_n$ can be expressed as\n",
        "### <center> $ùö´\\vec{w}=Œ∑\\frac{1}{N}\\sum_{n=1}^N ‚àáe_n(\\vec{w})$\n",
        "### So the updated weight vector is given by \n",
        "### <center> $\\vec{w}(t+1)=\\vec{w}(t)-Œ∑‚àáe_n(\\vec{w})$\n",
        "### which for $Œ∑=1$ is simply\n",
        "###  <center> $\\vec{w}(t+1)=\\vec{w}(t)-‚àáe_n(\\vec{w})$\n",
        "\n",
        "### In the case where the pt. is correctly classified, $e_n=\\nabla e_n=0$. When the pt. is misclassified, $e_n > 0$ and $‚àáe_n=-y_n\\vec{x}_n$. So in the two cases we get the update rules of \n",
        "### If correctly classified: <center> $\\vec{w}(t+1)=\\vec{w}(t)$\n",
        "### If misclassified: <center> $\\vec{w}(t+1)=\\vec{w}(t)-y_n\\vec{x}_n$\n",
        "### which are the same update rules that are used in the PLA. \n",
        "\n",
        "### b. For logisitc regression the derivative of the error measure is \n",
        "### <center> $\\nabla e_n(\\vec{w})=\\frac{-y_n\\vec{x}_n}{1+e^{y_n\\vec{w}^T\\vec{x}_n}}$. \n",
        "\n",
        "### Let's look at the two cases when $|\\vec{w}|>>0$.\n",
        "### For $\\vec{w}>>0$, the exponential term diverges, so $\\nabla e_n ‚âà0$.\n",
        "### For $\\vec{w}<<0$, the exponential term decays to zero, so $\\nabla e_n ‚âà-y_n\\vec{x}_n$\n",
        "\n",
        "### These two limits in the case of a large weight vector agree with the update rules of the PLA algorithm. When a pt. is correctly classified $e_n ‚âà0$ and the weight vector is not updated. When the pt. is misclassified, $e_n ‚âà-y_n\\vec{x}_n$ and the weight vector is updated by the same rule as he PLA.\n"
      ],
      "metadata": {
        "id": "Woby1t7OprzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EX. 4.2\n",
        "\n",
        "### This was a reading exercise about a theoretical experiment setup."
      ],
      "metadata": {
        "id": "N7RTtYzfqvdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern Recognition and Machine Learning "
      ],
      "metadata": {
        "id": "ZJ3Vuko5H7nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.9\n",
        "\n",
        "### The desired result is simply a scaling and shift of the binary outputs, which gives a new activation function\n",
        "### <center> $y=2œÉ(a)-1$\n",
        "### plugging in the sigmoid gives\n",
        "### <center> $y=\\frac{2}{1+e^{-a}}-1=\\frac{1-e^{-a}}{1+e^{-a}}=tanh(a/2)$\n",
        "### The original error function is given by eq. 5.21\n",
        "### <center> $E(\\vec{w})=-\\sum_{n=1}^{N} \\{t_nln(y_n)+(1-t_n)ln(1-y_n)\\}$\n",
        "### The new error function is given by applying the inverse of the activation function to $y_n$ and $t_n$ and plugging into eq. 5.21 to get\n",
        "### <center> $E(\\vec{w})=-\\sum_{n=1}^{N} \\{\\frac{1+t_n}{2}ln(\\frac{1+y_n}{2})+(1-\\frac{1+t_n}{2})ln(1-\\frac{1+y_n}{2})\\}$\n",
        "### <center> $=-\\frac{1}{2}\\sum_{n=1}^{N} \\{(1+t_n)ln(1+y_n)+(1-t_n)ln(1-y_n)\\}+Nln(2)$\n"
      ],
      "metadata": {
        "id": "1UGS52yMIAvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.28\n",
        "\n",
        "### The modification only affect the derivatives with respect to the weights in the convolutional layer. The units of a feature map with index m have differing inputs, but the same weight vector $\\vec{w}^{(m)}$. It follows that the errors $Œ¥^{(m)}$ from all units within a feature map contribute to the derivatives of the corresponding weight vector. The usual derivative of the error function from eq. 5.50 \n",
        "### <center> $\\frac{‚àÇE_n}{‚àÇw_{ji}}=\\frac{‚àÇE_n}{‚àÇa_j}\\frac{‚àÇa_j}{‚àÇw_{ji}}$\n",
        "### becomes \n",
        "### <center> $\\frac{‚àÇE_n}{‚àÇw^{(m)}_i}=\\sum_j \\frac{‚àÇE_n}{‚àÇa_j^{(m)}}\\frac{‚àÇa_j^{(m)}}{‚àÇw_{i}^{(m)}}=\\sum_j Œ¥_j^{(m)}z_{ji}^{(m)}$\n",
        "### where $a_j^{(m)}$ is the activation of the $j^{th}$ unit in the $m^{th}$ feature map, $w_i^{(m)}$ is the $i^{th}$ element of the corresponding feature vector, and $z_{ji}^{(m)}$ denotes the $i^{th}$ input fpr the $j^{th}$ unit in the $m^{th}$ feature map.\n",
        "\n"
      ],
      "metadata": {
        "id": "a7sqKyR_IE_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VesdK7vJrnoh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}