{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS4.BAYER.ZANE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Set 4 - Zane Bayer - CSCI 520 - 4/26/2022"
      ],
      "metadata": {
        "id": "3k0sz2E4awQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Pattern Recognition and Machine Learning*"
      ],
      "metadata": {
        "id": "694gvDRKa9IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1\n",
        "\n",
        "### The hyperbolic tangent function is given by \n",
        "### <center> $tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
        "### This can be rerranged into the form\n",
        "### <center> $tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
        "### <center> $=\\frac{e^x+e^{-x}-2e^{-x}}{e^x+e^{-x}}$\n",
        "### <center> $=1-\\frac{2e^{-x}}{e^x+e^{-x}}$\n",
        "### <center> $=1-\\frac{2e^{-x}}{e^{-x}(e^{2x}+1)}$\n",
        "### <center> $=1-\\frac{2}{e^{2x}+1}$\n",
        "### the sigmoid function is \n",
        "### <center> $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "### Comparing $\\sigma(x)$ and $tanh(x)$, we can see that they're related by \n",
        "### <center> $tanh(x)=1-\\frac{2}{e^{2x}+1}=1-2\\sigma(-2x)$\n",
        "### The sigmoid function obeys\n",
        "### <center> $\\sigma(-x)=1-\\sigma(x)$\n",
        "### So we get the reltion \n",
        "### <center> $tanh(x)=1-2\\sigma(-2x)=2\\sigma(2x)-1$\n",
        "### Eq.'s 5.[2-4] give the parameters for a 2 layer neural network with activation function $h(x)$\n",
        "### <center> $a_j=\\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)}$ for $j=1,...,M$\n",
        "### <center> $z_j=h(a_j)$\n",
        "### <center> $a_k=\\sum_{j=1}^M w_{kj}^{(2)}z_j+w_{k0}^{(2)}$ for $k=1,...,K$\n",
        "\n",
        "### Allow a network with tanh(x) activation functions to have weightd $[w_{ji}^{(1tanh)},w_{j0}^{(1tanh)},w_{kj}^{(2tanh)},w_{k0}^{(2tanh)}]$. Similarally, a network with sigmoid activations has weights of $[w_{ji}^{(1sig)},w_{j0}^{(1sig)},w_{kj}^{(2sig)},w_{k0}^{(2sig)}]$. For the network with tanh(x) activations, \n",
        "### <center> $a_k^{tanh}=∑_{j=1}^M w_{kj}^{(2tanh)}tanh(a_j^{tanh})+w_{k0}^{(2tanh)}$\n",
        "### We can now use the relation between tanh(x) and $σ(x)$ which we initially found and get \n",
        "### <center> $a_k^{tanh}=\\sum_{j=1}^M w_{kj}^{(2tanh)}[2σ(2a_j^{tanh})-1]+w_{k0}^{(2tanh)}$\n",
        "### <center> $=\\sum_{j=1}^M 2w_{kj}^{(2tanh)}σ(2a_j^{tanh})+[-\\sum_{j=1}^M w_{kj}^{(2tanh)}+ w_{k0}^{(2tanh)}]$\n",
        "### For the network with sigmoid activations, we have\n",
        "### <center> $a_k^{sig}=∑_{j=1}^M w_{kj}^{(2sig)}σ(a_j^{sig})+w_{k0}^{(2sig)}$\n",
        "### Equating the 2 networks, $a_k^{tanh}=a_k^{sig}$, and comparing the terms leads to the following conditions\n",
        "### <center> $2w_{kj}^{(2tanh)}=w_{kj}^{(2sig)}$\n",
        "### <center> $2a_j^{tanh}=a_j^{sig}$\n",
        "### <center> $-\\sum_{j=1}^M w_{kj}^{(2tanh)}+ w_{k0}^{(2tanh)}=w_{k0}^{(2sig)}$\n",
        "### the conditions describe a simple linear transformation between the 2 networks. Thus, the 2 networks are equivalent under a linear transformation."
      ],
      "metadata": {
        "id": "ofCMldz6uEdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2\n",
        "\n",
        "### the conditional distribution 5.16 is\n",
        "### <center> $p(\\vec{t}|\\vec{x},\\vec{w})=N(\\vec{t}|\\vec{y}(\\vec{x},\\vec{w}),\\beta^{-1}I)$\n",
        "### The negative logarithm of 5.16 is \n",
        "### <center>  $-ln[p(\\vec{t}|\\vec{x},\\vec{w})]=\\frac{\\beta }{2}\\sum_{n=1}^{N}[(y(x_n,\\vec{w})-\\vec{t}_n)^T(\\vec{y}(x_n,\\vec{w})-\\vec{t}_n)]+const.$\n",
        "\n",
        "### where the constant terms are independent of $\\vec{w}$. Eq. 5.11 is \n",
        "### <center> $E(\\vec{w})=\\frac{1}{2}\\sum_{n=1}^{N}||\\vec{y}(\\vec{x}_n,\\vec{w})-\\vec{t}_n||^2$\n",
        "### Clearly the 1st term of the negative logarithm equation is $-\\beta E(\\vec{w})$, so maximizing the the log-liklihood is equivalent to minimizing the sum-of-squares error. "
      ],
      "metadata": {
        "id": "4E14Oq1quHVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5\n",
        "\n",
        "### We're told that the interpretation of the network output is \n",
        "### <center> $y_k(\\vec{x},\\vec{w})=p(t_k=1|\\vec{x})$\n",
        "### For a given interpretation $y_k$, the conditional distribution of the target vector is\n",
        "### <center> $p(\\vec{t}|\\vec{w},...,\\vec{w}_k)=\\Pi_{k=1}^K y_k^{tk}$\n",
        "### for a dataset with N pt.'s, we get \n",
        "### <center> $p(T|\\vec{w}_1,...,\\vec{w}_k)=\\Pi_{n=1}^N \\Pi_{k=1}^K y_k^{tk}$\n",
        "### Taking the negative log of both sides gives the desired result 5.24,\n",
        "### <center> $-ln[p(T|\\vec{w}_1,...,\\vec{w}_k)]=-ln[\\Pi_{n=1}^N \\Pi_{k=1}^K y_k^{tk}]$\n",
        "### <center> $⇒ E(\\vec{w})=-\\sum_{n=1}^N \\sum_{k=1}^K ln(y_{nk}^{t_{nk}})$\n",
        "### <center> $=-\\sum_{n=1}^N \\sum_{k=1}^K t_{kn}ln(y(\\vec{x}_n,\\vec{w}))$\n"
      ],
      "metadata": {
        "id": "BnyYAzW00PH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6\n",
        "\n",
        "### eq. 5.21 is a cross-entropy error function of the form \n",
        "### <center> $E(\\vec{w})=-\\sum_{n=1}^{N}\\{t_nln(y_n)+(1-t_n)ln(1-y_n)\\}$\n",
        "### Taking the derivative with respect to $a_n$ gives\n",
        "### <center> $\\frac{∂E}{∂a_n}=\\frac{∂}{∂a_n}[-\\sum_{n=1}^{N}\\{t_nln(y_n)+(1-t_n)ln(1-y_n)\\}]$\n",
        "### <center> $=-\\frac{t_n}{y_n}\\frac{∂y_n}{∂a_n}+(1-t_n)\\frac{1}{1-y_n}\\frac{\\partial y_n}{∂a_n}$\n",
        "### Since the activation function is the sigmoid, we know $\\frac{\\partial y_n}{∂a_n}=y_n(1-y_n)$, so \n",
        "### <center>  $\\frac{∂E}{∂a_n}=-\\frac{t_n}{y_n}[y_n(1-y_n)+(1-t_n)(\\frac{1}{1-y_n})[y_n(1-y_n)]$\n",
        "### <center> $-t_n(1-y_n)+(1-t_n)y_n=y_n-t_n$\n",
        "### which is equivalent to the desired result eq. 5.18\n",
        "### <center> $\\frac{∂E}{∂a_k}=y_k-t_k$\n"
      ],
      "metadata": {
        "id": "rwSeTMkcGKb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.7\n",
        "\n",
        "### Taking the derivative of eq. 5.21 (see problem 5.6) with respect to $a_k$ gives\n",
        "### <center> $\\frac{∂E(\\vec{w})}{∂a_k}=\\frac{∂}{∂a_k}[-\\sum_{n=1}^N \\sum_{k=1}^K t_{kn}ln(y_k(x_n,\\vec{w}))]=-\\sum_{n=1}^N \\sum_{k=1}^K \\frac{∂}{∂a_k}[t_{kn}ln(y_k)]$\n",
        "### Since the activation is a softmax function, we know $\\frac{\\partial y_{kn}}{\\partial a_k}=y_{kn}(I_{kj}-y_{jn})$ where I is the identity matrix. So we have\n",
        "### <center> $\\frac{\\partial E}{∂a_k}=-\\sum_{n=1}^N \\sum_{k=1}^K\\frac{t_{kn}}{y_{kn}}[y_{kn}(I_{kj}-y_{jn})]$\n",
        "### <center> $=-\\sum_{n=1}^N \\sum_{k=1}^K [t_{kn}I_{kj}-t_{kn}y_{jn}]$\n",
        "### <center> $=-\\sum_{n=1}^N t_{jn}+\\sum_{n=1}^N y_{jn}$\n",
        "### <center> $=\\sum_{n=1}^N (y_{jn}-t_{jn})$\n",
        "### which satifies eq. 5.18\n",
        "### <center> $\\frac{∂E}{∂a_k}=y_k-t_k$\n",
        "### for each value of n in the summation."
      ],
      "metadata": {
        "id": "vKWCTLseQMC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.8\n",
        "\n",
        "### The derivative of the hyperbolic tangent function is \n",
        "### <center> $\\frac{d}{dx}tanh(x)=\\frac{d}{dx}(\\frac{e^x-e^{-x}}{e^x+e^{-x}})$\n",
        "### <center> $=\\frac{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}$\n",
        "### <center> $\\frac{(e^x+e^{-x})^2-(e^x-e^{-x})^2}{(e^x+e^{-x})^2}$\n",
        "### <center> $1-\\frac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2}$\n",
        "### <center> $1-tanh(x)^2$\n",
        "### So the relation is given by\n",
        "### <center> $\\frac{d}{dx}tanh(x)=1-[tanh(x)]^2$"
      ],
      "metadata": {
        "id": "TgqpS4YJLpg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.18\n",
        "\n",
        "### Section 5.3.2 decribes the parameters, error function, and derivatives of the error function for a 2 layer network.\n",
        "### Parameters: <center> $a_j=\\sum_{i=0}^D w_{ji}^{(1)}x_i$\n",
        "### <center> $z_j=h(a_j)$ for some activation function h(x)\n",
        "### <center> $y_k=∑_{j=0}^M w_{kj}^{(2)}z_j$\n",
        "### Error Function: <center> $E_n=\\frac{1}{2}∑_{k=1}^K (y_k-t_k)^2$\n",
        "### output layer $δ$: <center> $δ_k=y_k-t_k$\n",
        "### hidden layer $δ$: <center> $δ_j=(1-z_j^2)∑_{k=1}^K w_{kj}δ_k$\n",
        "### Error function derivatives: <center> $\\frac{∂E_n}{∂w_{ji}^{(1)}}=δ_jx_i$\n",
        "### <center>  $\\frac{∂E_n}{∂w_{kj}^{(2)}}=δ_kz_j$\n",
        "### From the above, it's rather simple to construct the desired derivative. Allow the skip layer connections to be denoted as $w_{ki}^{(s)}$ which connect the i-th input to the k-th output. It's clear then that the derivative of the error function with respect to the these weights is \n",
        "### <center>  $\\frac{∂E_n}{∂w_{ki}^{(s)}}=δ_kx_i$\n",
        "### where $δ_k$ is the same as above."
      ],
      "metadata": {
        "id": "RbiIigqeTDsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FQYe-7zyuAYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}